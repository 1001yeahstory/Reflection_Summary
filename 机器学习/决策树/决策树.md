# 常见决策树

| 模型      | ID3      |     C4.5 |   CART   |
| :--------| :-------- | --------:| :------: |
| 结构   | 多叉树    |   多叉树 |  二叉树  |
| 特征选择   | 信息增益    |   信息增益率 |  Gini系数/均方差  |
| 连续值处理   | 不支持    |   支持 |  支持  |
| 缺失值处理   | 不支持    |   支持 |  支持  |
| 枝剪   | 不支持    |   支持 |  支持  |


# 简述决策树构建过程
1. 构建根节点，将所有训练数据都放在根节点
2. 选择一个最优特征，按照这一特征将训练数据集分割成子集，使得各个子集有一个在当前条件下最好的分类
3. 如果子集非空，或子集容量未小于最少数量，递归1，2步骤，直到所有训练数据子集都被正确分类或没有合适的特征为止

# 详述信息熵计算方法及存在问题
![](https://tva1.sinaimg.cn/large/006y8mN6gy1g925k1ns0yj305y018glg.jpg)

其中，D为数据全集，C为不同因变量类别的子集(传统意义上的y的种类)

# 详述信息增益计算方法
条件信息熵：在特征A给定的条件下对数据集D分类的不确定性：
![](https://tva1.sinaimg.cn/large/006y8mN6gy1g925puuv1cj308p01kdfr.jpg)

信息增益：知道特征A的信息而使类D的信息的不确定减少的程度（对称）：
I(D,A) = H(D)-H(D/A)

简而言之，就是在特征A下找到最合适的切分，使得在该切分下信息量的变换最大，更加稳定；但是这个有一个问题，对于类别天生较多的特征，模型更容易选中，因为特征类别较多，切分后的信息增益天生更大，更容易满足我们的原始假设

# 详述信息增益率计算方法
在信息增益计算的基础不变的情况下得到的：I(D,A) = H(D)-H(D/A)，同时还考虑了![](https://tva1.sinaimg.cn/large/006y8mN6gy1g925yr9z1vj306601f745.jpg),用划分的子集数上的熵来平衡了分类数过多的问题。

信息增益率：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9260xx5wej304b017q2r.jpg)

# Gini系数
Gini系数二分情况下：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g92705elt9j308000q744.jpg)

对于决策树样本D来说，![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9273px3t2j30a4018wee.jpg)

对于样本D，如果根据特征A的某个值，把D分成D1和D2，则在特征A的条件下，D的基尼系数为：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9274yiwhoj30iq02wmx8.jpg)

# ID3存在的问题
缺点：
- 存在偏向于选择取值较多的特征问题
- 连续值不支持
- 缺失值不支持
- 无法枝剪


# C4.5的改进点
- 主动进行的连续的特征离散化
    - 比如m个样本的连续特征A有m个，先set在order，再两两组合求中间值，以该值点作为划分的待选点
    - **连续特征可以再后序特征划分中仍可继续参与计算**
- 缺失问题优化
    - 训练：用所有未缺失的样本，和之前一样，计算每个属性的信息增益，但是这里的信息增益需要乘以一个系数（未缺失样本/总样本）
    - 预测：直接跳过该节点，并将此样本划入所有子节点，划分后乘以系数计算，系数为不缺失部分的样本分布
- 采用预枝剪

  